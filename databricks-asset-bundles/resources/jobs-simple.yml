# Simplified Job Definitions for Initial Deployment - Serverless

resources:
  jobs:
    # Daily Ingestion Pipeline - Notebooks Only (Serverless)
    ticketmaster_daily_ingestion:
      name: "[${var.environment}] Ticketmaster Daily Ingestion"
      description: "Daily ingestion of Ticketmaster API data through Bronze/Silver/Gold layers"

      email_notifications:
        on_failure:
          - tanner.waggoner@databricks.com

      max_concurrent_runs: 1
      timeout_seconds: 7200

      tasks:
        # Task 1: Bronze Layer Processing
        - task_key: process_bronze
          description: "Stream raw JSON to Bronze Delta tables using Auto Loader"

          notebook_task:
            notebook_path: /Workspace/Users/tanner.waggoner@databricks.com/tix-master/src/bronze/bronze_auto_loader
            source: WORKSPACE
            base_parameters:
              catalog: ${var.catalog}

          job_cluster_key: serverless_cluster

        # Task 2: Silver Layer Transformation
        - task_key: process_silver
          description: "Transform Bronze to normalized Silver tables with PK/FK constraints"
          depends_on:
            - task_key: process_bronze

          notebook_task:
            notebook_path: /Workspace/Users/tanner.waggoner@databricks.com/tix-master/src/silver/silver_transformations
            source: WORKSPACE
            base_parameters:
              catalog: ${var.catalog}

          job_cluster_key: serverless_cluster

        # Task 3: Gold Layer Star Schema
        - task_key: process_gold
          description: "Build Gold star schema with identity keys"
          depends_on:
            - task_key: process_silver

          notebook_task:
            notebook_path: /Workspace/Users/tanner.waggoner@databricks.com/tix-master/src/gold/gold_star_schema
            source: WORKSPACE
            base_parameters:
              catalog: ${var.catalog}

          job_cluster_key: serverless_cluster

      job_clusters:
        - job_cluster_key: serverless_cluster
          new_cluster:
            spark_version: "15.4.x-scala2.12"
            node_type_id: "Standard_E4ds_v5"
            num_workers: 0
            spark_conf:
              "spark.databricks.cluster.profile": "serverless"
              "spark.databricks.delta.optimizeWrite.enabled": "true"
              "spark.databricks.delta.autoCompact.enabled": "true"
            custom_tags:
              ResourceClass: "Serverless"
