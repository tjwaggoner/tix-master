# Job Definitions for Ticketmaster Medallion Architecture

resources:
  jobs:
    # Daily Ingestion Pipeline
    ticketmaster_daily_ingestion:
      name: "[${var.environment}] Ticketmaster Daily Ingestion"
      description: "Daily ingestion of Ticketmaster API data through Bronze/Silver/Gold layers"

      schedule:
        quartz_cron_expression: "0 0 2 * * ?"  # Daily at 2 AM
        timezone_id: "America/Los_Angeles"
        pause_status: "UNPAUSED"

      email_notifications:
        on_failure:
          - data-engineering@company.com

      max_concurrent_runs: 1
      timeout_seconds: 7200  # 2 hours

      tasks:
        # Task 1: API Ingestion
        - task_key: ingest_api_data
          description: "Fetch data from Ticketmaster API and land to UC Volume"

          python_wheel_task:
            package_name: ticketmaster_ingestion
            entry_point: main

          libraries:
            - pypi:
                package: requests
            - pypi:
                package: tenacity

          new_cluster:
            spark_version: "14.3.x-scala2.12"
            node_type_id: ${var.cluster_node_type}
            num_workers: 2
            spark_env_vars:
              TICKETMASTER_API_KEY: "{{secrets/ticketmaster/api_key}}"

        # Task 2: Bronze Layer Processing
        - task_key: process_bronze
          description: "Stream raw JSON to Bronze Delta tables using Auto Loader"
          depends_on:
            - task_key: ingest_api_data

          notebook_task:
            notebook_path: ../src/bronze/bronze_auto_loader
            base_parameters:
              catalog: ${var.catalog}

          new_cluster:
            spark_version: "14.3.x-scala2.12"
            node_type_id: ${var.cluster_node_type}
            num_workers: 4
            spark_conf:
              "spark.databricks.delta.optimizeWrite.enabled": "true"
              "spark.databricks.delta.autoCompact.enabled": "true"

        # Task 3: Silver Layer Transformation
        - task_key: process_silver
          description: "Transform Bronze to normalized Silver tables with PK/FK constraints"
          depends_on:
            - task_key: process_bronze

          notebook_task:
            notebook_path: ../src/silver/silver_transformations
            base_parameters:
              catalog: ${var.catalog}

          new_cluster:
            spark_version: "14.3.x-scala2.12"
            node_type_id: ${var.cluster_node_type}
            num_workers: 4

        # Task 4: Gold Layer Star Schema
        - task_key: process_gold
          description: "Build Gold star schema with identity keys"
          depends_on:
            - task_key: process_silver

          notebook_task:
            notebook_path: ../src/gold/gold_star_schema
            base_parameters:
              catalog: ${var.catalog}

          new_cluster:
            spark_version: "14.3.x-scala2.12"
            node_type_id: ${var.cluster_node_type}
            num_workers: 2

        # Task 5: Data Quality Checks
        - task_key: data_quality_checks
          description: "Run data quality validation"
          depends_on:
            - task_key: process_gold

          sql_task:
            warehouse_id: ${var.warehouse_id}
            query: |
              DECLARE total_checks INT;
              DECLARE failed_checks INT;
              DECLARE quality_score DECIMAL(5,2);
              DECLARE status STRING;

              CALL ${var.catalog}.gold.sp_data_quality_checks(total_checks, failed_checks, quality_score, status);

              SELECT
                total_checks,
                failed_checks,
                quality_score,
                status;

        # Task 6: Refresh Gold Layer
        - task_key: refresh_gold
          description: "Execute stored procedure to refresh Gold layer"
          depends_on:
            - task_key: data_quality_checks

          sql_task:
            warehouse_id: ${var.warehouse_id}
            query: |
              DECLARE rows_processed INT;
              DECLARE status STRING;

              CALL ${var.catalog}.gold.sp_refresh_gold_layer(
                CURRENT_DATE() - INTERVAL 7 DAYS,
                CURRENT_DATE(),
                rows_processed,
                status
              );

              SELECT rows_processed, status;

        # Task 7: Update Vector Index for RAG
        - task_key: update_vector_index
          description: "Sync vector search index for RAG assistant"
          depends_on:
            - task_key: refresh_gold

          sql_task:
            warehouse_id: ${var.warehouse_id}
            query:
              query: |
                -- Refresh event documents
                INSERT OVERWRITE ${var.catalog}.gold.event_documents
                SELECT
                  f.event_id,
                  f.event_sk,
                  f.event_name,
                  f.event_type,
                  d.date_value as event_date,
                  v.venue_name,
                  v.city,
                  v.state,
                  v.country,
                  a.attraction_name,
                  a.genre_name,
                  a.segment_name,
                  f.price_min,
                  f.price_max,
                  f.price_currency,
                  CONCAT_WS(' | ',
                    CONCAT('Event: ', f.event_name),
                    CONCAT('Type: ', f.event_type),
                    CONCAT('Date: ', CAST(d.date_value AS STRING)),
                    CONCAT('Venue: ', v.venue_name),
                    CONCAT('Location: ', v.city, ', ', v.state, ', ', v.country),
                    CONCAT('Attraction: ', COALESCE(a.attraction_name, 'N/A')),
                    CONCAT('Genre: ', COALESCE(a.genre_name, 'N/A')),
                    CONCAT('Price Range: ', CAST(f.price_min AS STRING), '-', CAST(f.price_max AS STRING), ' ', f.price_currency)
                  ) as event_text
                FROM ${var.catalog}.gold.fact_events f
                INNER JOIN ${var.catalog}.gold.dim_date d ON f.event_date_key = d.date_key
                INNER JOIN ${var.catalog}.gold.dim_venue v ON f.venue_sk = v.venue_sk
                LEFT JOIN ${var.catalog}.gold.dim_attraction a ON f.attraction_sk = a.attraction_sk
                WHERE f.is_test = FALSE;

                -- Sync vector index
                ALTER INDEX ${var.catalog}.gold.events_index SYNC;

    # Weekly Summary Report Generation
    ticketmaster_weekly_summary:
      name: "[${var.environment}] Ticketmaster Weekly Summary"
      description: "Generate weekly summary reports"

      schedule:
        quartz_cron_expression: "0 0 6 ? * MON"  # Monday at 6 AM
        timezone_id: "America/Los_Angeles"

      tasks:
        - task_key: generate_summary
          sql_task:
            warehouse_id: ${var.warehouse_id}
            query:
              query: |
                DECLARE total_reports INT;
                DECLARE status STRING;

                CALL ${var.catalog}.gold.sp_generate_event_summary(
                  YEAR(CURRENT_DATE()),
                  total_reports,
                  status
                );

                SELECT total_reports, status;

    # Monthly Maintenance Job
    ticketmaster_monthly_maintenance:
      name: "[${var.environment}] Ticketmaster Monthly Maintenance"
      description: "Optimize tables, vacuum old files, and update statistics"

      schedule:
        quartz_cron_expression: "0 0 3 1 * ?"  # 1st of month at 3 AM
        timezone_id: "America/Los_Angeles"

      tasks:
        - task_key: optimize_tables
          sql_task:
            warehouse_id: ${var.warehouse_id}
            query:
              query: |
                -- Optimize Bronze tables
                OPTIMIZE ${var.catalog}.bronze.events_raw;
                OPTIMIZE ${var.catalog}.bronze.venues_raw;
                OPTIMIZE ${var.catalog}.bronze.attractions_raw;

                -- Optimize Silver tables
                OPTIMIZE ${var.catalog}.silver.events ZORDER BY (event_date);
                OPTIMIZE ${var.catalog}.silver.venues;
                OPTIMIZE ${var.catalog}.silver.attractions;

                -- Optimize Gold tables
                OPTIMIZE ${var.catalog}.gold.fact_events ZORDER BY (event_date_key, venue_sk);
                OPTIMIZE ${var.catalog}.gold.dim_venue;
                OPTIMIZE ${var.catalog}.gold.dim_attraction;
                OPTIMIZE ${var.catalog}.gold.dim_date;

        - task_key: vacuum_old_files
          depends_on:
            - task_key: optimize_tables
          sql_task:
            warehouse_id: ${var.warehouse_id}
            query:
              query: |
                -- Vacuum Bronze (keep 7 days)
                VACUUM ${var.catalog}.bronze.events_raw RETAIN 168 HOURS;
                VACUUM ${var.catalog}.bronze.venues_raw RETAIN 168 HOURS;

                -- Vacuum Silver (keep 14 days)
                VACUUM ${var.catalog}.silver.events RETAIN 336 HOURS;
                VACUUM ${var.catalog}.silver.venues RETAIN 336 HOURS;

                -- Vacuum Gold (keep 30 days)
                VACUUM ${var.catalog}.gold.fact_events RETAIN 720 HOURS;

        - task_key: update_statistics
          depends_on:
            - task_key: vacuum_old_files
          sql_task:
            warehouse_id: ${var.warehouse_id}
            query:
              query: |
                -- Update statistics for query optimization
                ANALYZE TABLE ${var.catalog}.gold.fact_events COMPUTE STATISTICS FOR ALL COLUMNS;
                ANALYZE TABLE ${var.catalog}.gold.dim_venue COMPUTE STATISTICS FOR ALL COLUMNS;
                ANALYZE TABLE ${var.catalog}.gold.dim_attraction COMPUTE STATISTICS FOR ALL COLUMNS;
                ANALYZE TABLE ${var.catalog}.gold.dim_date COMPUTE STATISTICS FOR ALL COLUMNS;
