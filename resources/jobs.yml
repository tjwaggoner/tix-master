# Databricks Jobs Configuration for Tix-Master Pipeline

resources:
  jobs:
    # Main ETL Pipeline Job
    tix_master_etl_pipeline:
      name: "[${bundle.target}] Tix Master ETL Pipeline"
      
      # Permissions
      permissions:
        - level: CAN_MANAGE
          user_name: cassi.sullivan@databricks.com
        - level: CAN_MANAGE
          user_name: alexander.booth@databricks.com
      
      # Email notifications
      email_notifications:
        on_failure:
          - ${workspace.current_user.userName}
      
      # Schedule: Run daily at 2 AM
      schedule:
        quartz_cron_expression: "0 0 2 ? * * *"
        timezone_id: "America/Los_Angeles"
        pause_status: "UNPAUSED"
      
      # No additional libraries needed - using serverless runtime built-ins
      
      # Pipeline tasks (using serverless compute)
      tasks:
        # Task 0: Ingest data from Ticketmaster API to Volume
        - task_key: ingest_ticketmaster_data
          notebook_task:
            notebook_path: ../src/ingestion/ticketmaster_ingestion_notebook.py
          timeout_seconds: 3600
        
        # Task 1: Load raw data from Volume to Bronze tables
        - task_key: bronze_auto_loader
          depends_on:
            - task_key: ingest_ticketmaster_data
          notebook_task:
            notebook_path: ../src/bronze/bronze_auto_loader.py
          timeout_seconds: 3600
        
        # Task 2: Transform data to Silver tables
        - task_key: silver_transformations
          depends_on:
            - task_key: bronze_auto_loader
          notebook_task:
            notebook_path: ../src/silver/silver_transformations.py
            base_parameters:
              catalog: "${var.catalog_name}"
              bronze_schema: "${var.bronze_schema}"
              silver_schema: "${var.silver_schema}"
          timeout_seconds: 3600
        
        # Task 3: Create data quality checks stored procedure
        - task_key: create_sp_data_quality
          depends_on:
            - task_key: silver_transformations
          sql_task:
            file:
              path: ../sql/stored_procedures/sp_data_quality_checks.sql
            warehouse_id: "${var.warehouse_id}"
          timeout_seconds: 1800
        
        # Task 4: Run data quality checks
        - task_key: call_data_quality_checks
          depends_on:
            - task_key: create_sp_data_quality
          sql_task:
            file:
              path: ../sql/stored_procedures/call_data_quality_checks.sql
            warehouse_id: "${var.warehouse_id}"
          timeout_seconds: 1800
        
        # Task 5: Build Gold star schema
        - task_key: gold_star_schema
          depends_on:
            - task_key: call_data_quality_checks
          notebook_task:
            notebook_path: ../src/gold/gold_star_schema.py
            base_parameters:
              catalog: "${var.catalog_name}"
              silver_schema: "${var.silver_schema}"
              gold_schema: "${var.gold_schema}"
          timeout_seconds: 3600
        
        # Task 6: Create event summary stored procedure
        - task_key: create_sp_event_summary
          depends_on:
            - task_key: gold_star_schema
          sql_task:
            file:
              path: ../sql/stored_procedures/sp_generate_event_summary.sql
            warehouse_id: "${var.warehouse_id}"
          timeout_seconds: 1800
        
        # Task 7: Run event summary generation
        - task_key: call_event_summary
          depends_on:
            - task_key: create_sp_event_summary
          sql_task:
            file:
              path: ../sql/stored_procedures/call_event_summary.sql
            warehouse_id: "${var.warehouse_id}"
          timeout_seconds: 1800
        
        # Task 8: Sync RAG vector index with new events
        # NOTE: This task will fail until you run src/ai/rag_assistant.py to create the vector index
        # Uncomment after creating the index manually
        # - task_key: sync_vector_index
        #   depends_on:
        #     - task_key: call_event_summary
        #   sql_task:
        #     file:
        #       path: ../src/ai/sync_vector_index.sql
        #     warehouse_id: "${var.warehouse_id}"
        #   timeout_seconds: 600
      
      # Retry policy
      max_concurrent_runs: 1
      timeout_seconds: 14400  # 4 hours

    ## TODO: To uncomment, please deploy to a classic workspace, not a serverless workspace.
    # # Creation of Genie Space
    # provision_genie_space:
    #   name: "[${bundle.target}] Provision Genie Space"
      
    #   permissions:
    #     - level: CAN_MANAGE
    #       user_name: cassi.sullivan@databricks.com
      
    #   tasks:
    #     - task_key: create_or_get_space
    #       spark_python_task:
    #         python_file: ../src/ai/create_genie_space.py
    #       new_cluster:
    #         spark_version: "15.4.x-scala2.12"
    #         node_type_id: "i3.xlarge"
    #         num_workers: 0
    #         spark_conf:
    #           "spark.databricks.cluster.profile": "singleNode"
    #           "spark.master": "local[*]"
    #         custom_tags:
    #           ResourceClass: "SingleNode"
    #       libraries:
    #         - pypi:
    #             package: requests>=2.31.0
    #         - pypi:
    #             package: PyYAML>=6.0.0
    #       # Pass inputs via environment variables; set these when running the job
      
    #   max_concurrent_runs: 1
    #   timeout_seconds: 1800
